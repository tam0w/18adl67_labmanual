{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Program 1\n",
    "\n",
    "A study was conducted to understand the effect of number of hours the students spent studying on their performance in the final exams. Write a code to plot line chart with number of hours spent studying on x-axis and score in final exam on y-axis. Use a red '*' as the point character, label the axes and give the plot a title\n",
    "\n",
    "No of hours spent Studying &emsp;&emsp;&emsp;|&nbsp;Score in final exam&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;&nbsp;|\n",
    "x===========================================x\n",
    "10&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;| 95&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|\n",
    "9&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;&nbsp;| 80&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|\n",
    "2&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;&nbsp;| 10&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|\n",
    "15&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;| 50&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|\n",
    "10&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;| 45&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|\n",
    "16&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;| 98&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|\n",
    "11&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;| 38&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|\n",
    "16&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;| 93&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "hours = [10,9,2,15,10,16,11,16]\n",
    "scores = [95,80,10,50,45,98,38,93]\n",
    "\n",
    "plt.xlabel('Hours Spent Studying')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Hours Studied vs Test Scores')\n",
    "\n",
    "plt.plot(hours,scores,marker=\"*\",color='r')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Program 2\n",
    "\n",
    "For the given dataset mtcars.csv, plot a histogram to check the frequency distribution of the variable \"MPG\" (Miles per gallon)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('datasets/mtcars.csv')\n",
    "\n",
    "plt.hist(df.mpg,bins=10,edgecolor='black')\n",
    "\n",
    "plt.xlabel('Miles per Gallon')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency Distribution of MPG')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Program 3\n",
    "\n",
    "Consider the books dataset books.csv from Kaggle which contains information about books. Write a program to demonstrate the following.\n",
    "1. Import the data into a DataFrame.\n",
    "2. Find and drop the columns which are irrelevant for the book information.\n",
    "3. Change the Index of the DataFrame.\n",
    "4. Tidy up fields in the data such as date of publication with the help of simple regular expression. Combine str methods with NumPy to clean columns\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datasets/books.csv')\n",
    "\n",
    "print('\\nColumns of the dataset:\\n',df.columns.values)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Relevant columns for me are: Title, Author, Date of Publication, Publisher\n",
    "df = df[['Identifier','Title','Author','Date of Publication','Publisher']]\n",
    "df.head()\n",
    "# You can also drop using df.drop([listofcolumns])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "df = df.set_index('Identifier')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "date = r'^\\[(\\d{4})\\]'\n",
    "years = df['Date of Publication'].str.extract(date,expand=False)\n",
    "years = pd.to_numeric(years)\n",
    "mean_years = np.mean(years)\n",
    "years = years.fillna(mean_years)\n",
    "df['Date of Publication'] = years\n",
    "\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Program 4\n",
    "\n",
    "Train a regularized logistic regression classifier on the iris dataset (https://archive.ics.uci.edu/ml/machine-learning-databases/iris/ or the inbuilt iris dataset) using sklearn. Train the model with the following hyperparameter C = 1e4 and report the best classification accuracy.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "\"Imports complete.\"\n",
    "iris = load_iris()\n",
    "y = iris.target\n",
    "X = iris.data\n",
    "class_names = iris.target_names\n",
    "\n",
    "Xtrain,Xtest,ytrain,ytest = train_test_split(X,y,test_size = 0.2, random_state =42)\n",
    "\n",
    "\"Training and test splits have been made\"\n",
    "scaler = StandardScaler()\n",
    "Xtrain = scaler.fit_transform(Xtrain)\n",
    "Xtest = scaler.transform(Xtest)\n",
    "\n",
    "\"Data has been rescaled\"\n",
    "C = 1e4\n",
    "model = LogisticRegression(C=C, penalty='l2',max_iter=1000,solver='liblinear')\n",
    "model.fit(Xtrain,ytrain)\n",
    "ypred = model.predict(Xtest)\n",
    "\"Model created.\"\n",
    "print(\"Accuracy based on test set:\", accuracy_score(ypred,ytest))\n",
    "cm = confusion_matrix(ytest,ypred)\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(cm,annot=True,cmap='Greys',xticklabels=class_names,yticklabels=class_names,fmt='d')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Actual Values')\n",
    "plt.show()\n",
    "sample = np.array([[5,3,1,0]])\n",
    "ysample = model.predict(sample)\n",
    "predclass = class_names[ysample[0]]\n",
    "print(\"Predicted Class:\" ,predclass)\n",
    "\n",
    "model_dict = {\n",
    "    \"coef\": model.coef_.tolist(),\n",
    "    \"intercept\": model.intercept_.tolist(),\n",
    "    \"classes\": iris.target_names.tolist(),\n",
    "    \"attributes\": iris.feature_names\n",
    "}\n",
    "\n",
    "with open(\"model.json\",'w') as jsonlover:\n",
    "    json.dump(model_dict,jsonlover)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Program 5\n",
    "\n",
    "Train an SVM classifier on the iris dataset using sklearn. Try different kernels and the associated hyperparameters. Train model with the following set hyperparameters RBF-kernel, gamma=0.5, onevs-rest classifier, no-feature-normalization. Also try C=0.01,1,10C=0.01,1,10. For the above set of hyperparameters, find the best classification accuracy along with total number of support vectors on the test data.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "import pickle\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "class_names = iris.target_names\n",
    "Xtrain,Xtest,ytrain,ytest = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "gammas = [0.5]\n",
    "Cc = [0.01,1,10]\n",
    "kernels = ['rbf']\n",
    "\n",
    "best_accuracy = 0\n",
    "best_sup_vectors = 0\n",
    "colors = ['Blues','Reds','Greys']\n",
    "\n",
    "for kernel in kernels:\n",
    "    for gamma in gammas:\n",
    "        for colour_no,C in enumerate(Cc):\n",
    "            colour = colors[colour_no]\n",
    "            model = SVC(kernel=kernel,gamma=gamma,C=C)\n",
    "            model.fit(Xtrain,ytrain)\n",
    "            ypred = model.predict(Xtest)\n",
    "            print(\"Model Info: \\nKernel, Gamma, C\")\n",
    "            print(kernel,gamma,C,sep='\\t\\t')\n",
    "\n",
    "            accuracy=accuracy_score(ytest,ypred)\n",
    "\n",
    "            print(\"Accuracy:\",accuracy)\n",
    "            print('-----------------')\n",
    "\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_sup_vectors = model.n_support_.sum()\n",
    "\n",
    "            cm = confusion_matrix(ytest,ypred)\n",
    "            plt.figure(figsize=(4,4))\n",
    "            sns.heatmap(cm,annot=True,cmap=colour,fmt='d')\n",
    "            plt.xlabel('Actual Class')\n",
    "            plt.ylabel('Predicted Class')\n",
    "            plt.show()\n",
    "\n",
    "print(\"Best Accuracy AND Number of Support Vectors:\",best_sup_vectors,\"and\",best_accuracy)\n",
    "\n",
    "with open('model.pkl','wb') as pkl:\n",
    "    pickle.dump(model,pkl)\n",
    "\n",
    "with open('model.pkl','rb') as pkl:\n",
    "    loaded_model = pickle.load(pkl)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Program 6\n",
    "\n",
    "Consider the following dataset. Write a program to demonstrate the working of the decision tree based ID3 algorithm.\n",
    "Headers = price, maintanence, capacity, airbag, profitable\n",
    "                        [['low', 'low', 1, 'no', 'no'],\n",
    "                                    ['low', 'low', 2, 'no', 'no'],\n",
    "                                    ['med', 'low', 2, 'yes', 'yes'],\n",
    "                                    ['high', 'med', 2, 'yes', 'yes'],\n",
    "                                    ['high', 'high', 3, 'yes', 'yes'],\n",
    "                                    ['high', 'high', 3, 'no', 'no'],\n",
    "                                    ['med', 'high', 3, 'yes', 'no']]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9852281360342516, 0.5916727785823276, 0.9852281360342516, 0.9852281360342516]\n",
      "[0.0, 1.0, 1.0]\n",
      "[0.0, 0.9182958340544896, 0.9182958340544896]\n",
      "[0.0, 0.0]\n",
      "[0.0]\n",
      "[]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[37], line 88\u001B[0m\n\u001B[0;32m     86\u001B[0m attributes \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPrice\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmaintenance\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcapacity\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mairbag\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     87\u001B[0m class_labels \u001B[38;5;241m=\u001B[39m [sample[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m sample \u001B[38;5;129;01min\u001B[39;00m dataset]\n\u001B[1;32m---> 88\u001B[0m tree \u001B[38;5;241m=\u001B[39m \u001B[43mbuild\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattributes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclass_labels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     90\u001B[0m test \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlow\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlow\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m1\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlow\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     91\u001B[0m ans \u001B[38;5;241m=\u001B[39m predict(test,tree)\n",
      "Cell \u001B[1;32mIn[37], line 60\u001B[0m, in \u001B[0;36mbuild\u001B[1;34m(dataset, attributes, class_labels)\u001B[0m\n\u001B[0;32m     58\u001B[0m     remaining \u001B[38;5;241m=\u001B[39m attributes[:]\n\u001B[0;32m     59\u001B[0m     remaining\u001B[38;5;241m.\u001B[39mremove(best_attr)\n\u001B[1;32m---> 60\u001B[0m     tree[best_attr][value] \u001B[38;5;241m=\u001B[39m \u001B[43mbuild\u001B[49m\u001B[43m(\u001B[49m\u001B[43msubset\u001B[49m\u001B[43m,\u001B[49m\u001B[43mremaining\u001B[49m\u001B[43m,\u001B[49m\u001B[43msubset_class_labels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tree\n",
      "Cell \u001B[1;32mIn[37], line 60\u001B[0m, in \u001B[0;36mbuild\u001B[1;34m(dataset, attributes, class_labels)\u001B[0m\n\u001B[0;32m     58\u001B[0m     remaining \u001B[38;5;241m=\u001B[39m attributes[:]\n\u001B[0;32m     59\u001B[0m     remaining\u001B[38;5;241m.\u001B[39mremove(best_attr)\n\u001B[1;32m---> 60\u001B[0m     tree[best_attr][value] \u001B[38;5;241m=\u001B[39m \u001B[43mbuild\u001B[49m\u001B[43m(\u001B[49m\u001B[43msubset\u001B[49m\u001B[43m,\u001B[49m\u001B[43mremaining\u001B[49m\u001B[43m,\u001B[49m\u001B[43msubset_class_labels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tree\n",
      "    \u001B[1;31m[... skipping similar frames: build at line 60 (1 times)]\u001B[0m\n",
      "Cell \u001B[1;32mIn[37], line 60\u001B[0m, in \u001B[0;36mbuild\u001B[1;34m(dataset, attributes, class_labels)\u001B[0m\n\u001B[0;32m     58\u001B[0m     remaining \u001B[38;5;241m=\u001B[39m attributes[:]\n\u001B[0;32m     59\u001B[0m     remaining\u001B[38;5;241m.\u001B[39mremove(best_attr)\n\u001B[1;32m---> 60\u001B[0m     tree[best_attr][value] \u001B[38;5;241m=\u001B[39m \u001B[43mbuild\u001B[49m\u001B[43m(\u001B[49m\u001B[43msubset\u001B[49m\u001B[43m,\u001B[49m\u001B[43mremaining\u001B[49m\u001B[43m,\u001B[49m\u001B[43msubset_class_labels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tree\n",
      "Cell \u001B[1;32mIn[37], line 51\u001B[0m, in \u001B[0;36mbuild\u001B[1;34m(dataset, attributes, class_labels)\u001B[0m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(attributes) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m     49\u001B[0m     majority(class_labels)\n\u001B[1;32m---> 51\u001B[0m best_attr \u001B[38;5;241m=\u001B[39m \u001B[43mget_best\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43mattributes\u001B[49m\u001B[43m,\u001B[49m\u001B[43mclass_labels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     52\u001B[0m tree \u001B[38;5;241m=\u001B[39m {best_attr: {}}\n\u001B[0;32m     53\u001B[0m attribute_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(sample[attributes\u001B[38;5;241m.\u001B[39mindex(best_attr)] \u001B[38;5;28;01mfor\u001B[39;00m sample \u001B[38;5;129;01min\u001B[39;00m dataset)\n",
      "Cell \u001B[1;32mIn[37], line 33\u001B[0m, in \u001B[0;36mget_best\u001B[1;34m(dataset, attributes, class_labels)\u001B[0m\n\u001B[0;32m     31\u001B[0m gains \u001B[38;5;241m=\u001B[39m [info_gain(dataset,attribute,class_labels) \u001B[38;5;28;01mfor\u001B[39;00m attribute \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(attributes))]\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28mprint\u001B[39m(gains)\n\u001B[1;32m---> 33\u001B[0m best_index \u001B[38;5;241m=\u001B[39m gains\u001B[38;5;241m.\u001B[39mindex(\u001B[38;5;28;43mmax\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mgains\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m attributes[best_index]\n",
      "\u001B[1;31mValueError\u001B[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def entropy(class_labels):\n",
    "    total_samples = len(class_labels)\n",
    "    if total_samples == 0:\n",
    "        return 0\n",
    "    class_counts = Counter(class_labels).values()\n",
    "    class_probabilities = [count / total_samples for count in class_counts]\n",
    "    entropy_value = -sum(p * math.log2(p) for p in class_probabilities)\n",
    "    return entropy_value\n",
    "\n",
    "\n",
    "def information_gain(dataset, attribute, class_labels):\n",
    "    total_samples = len(dataset)\n",
    "    attribute_values = set(sample[attribute] for sample in dataset)\n",
    "    attribute_entropy = 0\n",
    "\n",
    "    for value in attribute_values:\n",
    "        subset = [sample for sample in dataset if sample[attribute] == value]\n",
    "        subset_class_labels = [sample[-1] for sample in subset]\n",
    "        subset_entropy = entropy(subset_class_labels)\n",
    "        subset_ratio = len(subset) / total_samples\n",
    "        attribute_entropy += subset_ratio * subset_entropy\n",
    "    gain = entropy(class_labels) - attribute_entropy\n",
    "    return gain\n",
    "\n",
    "\n",
    "def get_best_attribute(dataset, attributes, class_labels):\n",
    "    gains = [information_gain(dataset, attribute, class_labels) for attribute in range(len(attributes))]\n",
    "    best_attribute_index = gains.index(max(gains))\n",
    "    return attributes[best_attribute_index]\n",
    "\n",
    "\n",
    "def majority_class(class_labels):\n",
    "    count_yes = class_labels.count('yes')\n",
    "    count_no = class_labels.count('no')\n",
    "    if count_yes >= count_no:\n",
    "        return \"Yes\"\n",
    "    else:\n",
    "        return \"No\"\n",
    "\n",
    "\n",
    "def build_decision_tree(dataset, attributes, class_labels):\n",
    "    if len(set(class_labels)) == 1:\n",
    "        return class_labels[0]\n",
    "\n",
    "    if len(attributes) == 0:\n",
    "        return majority_class(class_labels)\n",
    "\n",
    "    best_attribute = get_best_attribute(dataset, attributes, class_labels)\n",
    "    tree = {best_attribute: {}}\n",
    "    attribute_values = set(sample[attributes.index(best_attribute)] for sample in dataset)\n",
    "\n",
    "    for value in attribute_values:\n",
    "        subset = [sample for sample in dataset if sample[attributes.index(best_attribute)] == value]\n",
    "        subset_class_labels = [sample[-1] for sample in subset]\n",
    "        remaining_attributes = attributes[:]\n",
    "        remaining_attributes.remove(best_attribute)\n",
    "        tree[best_attribute][value] = build_decision_tree(subset, remaining_attributes, subset_class_labels)\n",
    "    return tree\n",
    "\n",
    "\n",
    "def predict(sample, decision_tree):\n",
    "    if isinstance(decision_tree, str):\n",
    "        return decision_tree\n",
    "    attribute = next(iter(decision_tree))\n",
    "    value = sample[attributes.index(attribute)]\n",
    "\n",
    "    if value not in decision_tree[attribute]:\n",
    "        return None\n",
    "    subtree = decision_tree[attribute][value]\n",
    "    return predict(sample, subtree)\n",
    "\n",
    "\n",
    "dataset = [\n",
    "    ['low', 'low', 1, 'no', 'no'],\n",
    "    ['low', 'low', 2, 'no', 'no'],\n",
    "    ['med', 'low', 2, 'yes', 'yes'],\n",
    "    ['high', 'med', 2, 'yes', 'yes'],\n",
    "    ['high', 'high', 3, 'yes', 'yes'],\n",
    "    ['high', 'high', 3, 'no', 'no'],\n",
    "    ['med', 'high', 3, 'yes', 'no']\n",
    "]\n",
    "attributes = ['Price', 'maintenance', 'capacity', 'airbag']\n",
    "class_labels = [sample[-1] for sample in dataset]\n",
    "decision_tree = build_decision_tree(dataset, attributes, class_labels)\n",
    "\n",
    "sample = ['low', 'low', 1, 'no']\n",
    "prediction = predict(sample, decision_tree)\n",
    "\n",
    "print(\"prediction:\", prediction)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-19T15:59:38.093105600Z",
     "start_time": "2023-07-19T15:59:38.022210700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Program 7\n",
    "\n",
    "Consider the dataset spiral.txt (https://bit.ly/2Lm75Ly). The first two columns in the dataset corresponds to the co-ordinates of each data point. The third column corresponds to the actual cluster label. Compute the rand index for the following methods: 1. K – means Clustering 2. Single – link Hierarchical Clustering 3. Complete link hierarchical clustering. Also visualize the dataset and which algorithm will be able to recover the true clusters.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "df = pd.read_csv('datasets/Spiral.txt', skiprows=1,header=None, delimiter='\\t')\n",
    "df.columns = ['f1','f2','labels']\n",
    "X = df[['f1','f2']].astype(float).values\n",
    "y = df['labels'].values\n",
    "\n",
    "km = KMeans(random_state=42,n_clusters=3)\n",
    "ypredkm = km.fit_predict(X,y)\n",
    "kmscore = adjusted_rand_score(y,ypredkm)\n",
    "\n",
    "sg = AgglomerativeClustering(n_clusters=3, linkage='single')\n",
    "ypredsg = sg.fit_predict(X,y)\n",
    "sgscore = adjusted_rand_score(y,ypredsg)\n",
    "\n",
    "cm = AgglomerativeClustering(n_clusters=3, linkage='complete')\n",
    "ypredcm = cm.fit_predict(X,y)\n",
    "cmscore = adjusted_rand_score(y,ypredcm)\n",
    "\n",
    "fig, ax = plt.subplots(1,3,figsize=(15,7))\n",
    "ax[0].scatter(X[:,0],X[:,1],c=ypredkm)\n",
    "ax[0].set_title(f'Kmeans Clustering RI Score:{kmscore:.3f}')\n",
    "\n",
    "ax[1].scatter(X[:,0],X[:,1],c=ypredsg)\n",
    "ax[1].set_title(f'SL Clustering RI Score:{sgscore:.3f}')\n",
    "\n",
    "ax[2].scatter(X[:,0],X[:,1],c=ypredcm)\n",
    "ax[2].set_title(f'CL Clustering RI Score:{cmscore:.3f}')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Program 8\n",
    "\n",
    "Implement a k-Nearest Neighbor algorithm to classify the iris dataset. Print out both correct and wrong predictions.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "labels = iris.target_names\n",
    "\n",
    "k=3\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "xtrain,xtest,ytrain,ytest = train_test_split(x,y,random_state=42,test_size=0.2)\n",
    "\n",
    "knn.fit(xtrain,ytrain)\n",
    "\n",
    "ypred = knn.predict(xtest)\n",
    "\n",
    "for i in range(len(xtest)):\n",
    "    pred = labels[ypred[i]]\n",
    "    actual = labels[ytest[i]]\n",
    "    result = \"Correct\" if pred == actual else \"FALSE\"\n",
    "    print(f'The prediction is {pred} and the actual value is {actual} \\n{result}')\n",
    "    print('------------------------------------- \\n')\n",
    "\n",
    "print(\"Accuracy is:\",accuracy_score(ytest,ypred))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
